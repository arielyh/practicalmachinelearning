---
title: "Practical Machine Learning Course Project"
author: "Yue Hou"
date: "July 1, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#############################    project description    #################################

Use the "pml-training" dataset to predict the manner how 6 participants exercise.
I split the training data (19622 observations) into 60% of train (11776 samples),
20% of validation (3923) and 20% of test (3923). And used validation set for
model selection and parameter tuning, at last used test set to calculate the
out of sample error (accuracy for this multi-class classification problem).

first I cleaned and explored the train data set by removing the NA columns and
calculating the correlation among pairs of predictors. There are 20 pairs with
high correlations, so I tried PCA in my modeling methods.

Second, I did the same preprocessing with validation data set (removing NAs).
Then I tried 4 models (random forest, classificaiton tree, svm and boosting).
Since this is a classification problem, so I didn't choose regression models.
For the classification tree and svm, the accuracy on validation set is poor,
so I also tried PCA on these two methods, but no significant improvement.
By comparing the 6 methods (rf, tree +/- PCA, svm +/- PCA, boosting) on validation,
I decided to choose boosting (gbm) model, because it's as accurate as rf, but much faster.

Third, I tried to interpret the prediction variables, so I compared the variance importance
rank for each method, and plot some scatter and box plot for them vs. output "classe".

Finally, I used the test set (3923 samples, not the "pml-testing" 20 samples for the quiz)
to calculate the out of sample error.

Conclusion: I used 5-fold cross-validation, center and scale preprocessing for all the models,
PCA with 90% variance is not better than without PCA. After comparing 4 classification models
(random forest, svm, classification tree and boosting), I chose boosting because
it's as accurate as rf, but much faster than rf. SVM and CT are not very accurate, although
they are kind of fast. The out of sample error on my test_data2 is 0.9964 accuracy.

#############################             Codes            #################################
set up working directory
```{setwd("C:/Courses/Practicle ML in R caret")

# open the installed 'easypackages' library, which can install and open multiple packages simutaneously

library(easypackages)

# load all needed packages

libraries("ggplot2", "corrplot", "gridExtra", "Hmisc", "caTools", "caret", "gbm", "randomForest")}
```

###########    1. Data Cleaning and Exloration    #################################
load the training and testing data
```{training <- read.csv("~/pml-training.csv")
    testing <- read.csv("~/pml-testing.csv")}
```
split the training data into train_data (60%), validation_data (20%) and test_data (20%)
```{# set seed for reproducibility
set.seed(1234)
# first split training into train (60%) and inBuild (40%)
train_index <- createDataPartition(training$classe, p = 0.6, list = FALSE)
train_data <- training[train_index, ]   # train data set
inBuild_data <- training[-train_index, ]
# then split inBuild into validation (20%) and test (20%)
validation_index <- createDataPartition(inBuild_data$classe, p = 0.5, list = FALSE)
validation_data <- inBuild_data[validation_index, ]   # validation data set
test_data <- inBuild_data[-validation_index, ]        # test data set
}
# look at each data set's dimension
dim(train_data); dim(validation_data); dim(test_data)
```

```{## wragling train_data
# find all the predictors with NA or blank and then remove them from train_data
# create a substitude train_data2 and do cleaning on train_data2
train_data2 <- train_data
train_data2[train_data2 == c("", " ")] <- NA    # replace all blank with "NA"
train_data2 <- train_data2[, colSums(is.na(train_data2)) == 0]  # delete all columns with "NA"
}
# check the proportion for each class in train_data, 
# class A is twice size as others, but it's still pretty balanced for each class. 
prop.table(table(train_data$classe))
# check the predictors (names, type, etc) in train_data, many predictors have NA values.
str(train_data)
# explorating train_data2: the predictors structure/class, distribution/frequency
str(train_data2)
summary(train_data2)
```
col 3 and 7 looks weired, they have lots of same values.
```{# boxplot of them vs. output "classe"
png("Figure1.png",
    width = 5*500,        # 5 x 300 pixels
    height = 5*400,
    res = 300,            # 300 pixels per inch
    pointsize = 1)
p1 <- qplot(classe, raw_timestamp_part_1, data = train_data2, fill = classe, 
            geom = c("boxplot", "jitter"))
p2 <- qplot(classe, num_window, data = train_data2, fill = classe, 
            geom = c("boxplot", "jitter"))
grid.arrange(p1, p2, ncol = 2)
dev.off()}
p1 <- qplot(classe, raw_timestamp_part_1, data = train_data2, fill = classe, 
            geom = c("boxplot", "jitter"))
p2 <- qplot(classe, num_window, data = train_data2, fill = classe, 
            geom = c("boxplot", "jitter"))
grid.arrange(p1, p2, ncol = 2)
```


## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
